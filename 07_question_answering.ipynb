{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D34R/coding-adventure/blob/master/07_question_answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFLixVlMOnmc",
        "outputId": "02a1c039-ec91-4e6e-ef82-54089bf1512f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'notebooks'...\n",
            "remote: Enumerating objects: 526, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 526 (delta 143), reused 135 (delta 126), pack-reused 353\u001b[K\n",
            "Receiving objects: 100% (526/526), 28.62 MiB | 11.83 MiB/s, done.\n",
            "Resolving deltas: 100% (250/250), done.\n",
            "/content/notebooks/notebooks/notebooks/notebooks\n",
            "⏳ Installing base requirements ...\n",
            "✅ Base requirements installed!\n",
            "⏳ Installing Git LFS ...\n",
            "✅ Git LFS installed!\n"
          ]
        }
      ],
      "source": [
        "# Uncomment and run this cell if you're on Colab or Kaggle\n",
        "!git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "%cd notebooks\n",
        "from install import *\n",
        "install_requirements()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fksvS2hSOnme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68711855-a396-49ec-e1c8-e17b2ac66c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haystack\n",
            "  Downloading haystack-0.42-py2.py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting construct<2.8 (from haystack)\n",
            "  Downloading construct-2.5.3.tar.gz (688 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m688.1/688.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pefile (from haystack)\n",
            "  Downloading pefile-2023.2.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-ptrace>=0.8.1 (from haystack)\n",
            "  Downloading python_ptrace-0.9.9-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from construct<2.8->haystack) (1.16.0)\n",
            "Building wheels for collected packages: construct\n",
            "  Building wheel for construct (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for construct: filename=construct-2.5.3-py2.py3-none-any.whl size=71822 sha256=a17d91bbb46a6f2d900af7324c05c141c17105dd9c48a9e04352b4cd4b5225c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/16/f9/f48a4c1a687e0848495c1a95dd9a87a246974b9318240d140b\n",
            "Successfully built construct\n",
            "Installing collected packages: python-ptrace, pefile, construct, haystack\n",
            "Successfully installed construct-2.5.3 haystack-0.42 pefile-2023.2.7 python-ptrace-0.9.9\n",
            "Using transformers v4.16.2\n",
            "Using datasets v1.16.1\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "#!pip install utils\n",
        "\n",
        "from utils import *\n",
        "setup_chapter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2PkzzV35Onmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6021ff4-7801-4162-d432-bd453b065214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TOKENIZERS_PARALLELISM=false\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%env TOKENIZERS_PARALLELISM=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4EHdgx5hOnmg"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "# Suppress Haystack logging\n",
        "import logging\n",
        "for module in [\"farm.utils\", \"farm.infer\", \"haystack.reader.farm.FARMReader\",\n",
        "              \"farm.modeling.prediction_head\", \"elasticsearch\", \"haystack.eval\",\n",
        "               \"haystack.document_store.base\", \"haystack.retriever.base\",\n",
        "              \"farm.data_handler.dataset\"]:\n",
        "    module_logger = logging.getLogger(module)\n",
        "    module_logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1QaGPkLOnmg"
      },
      "source": [
        "# Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gv39FAzOnmh"
      },
      "source": [
        "<img alt=\"Marie Curie\" width=\"500\" caption=\"A Google search query and corresponding answer snippet\" src=\"images/chapter07_marie-curie.png\" id=\"marie-curie\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7KXqOLwOnmh"
      },
      "source": [
        "## Building a Review-Based QA System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8cLMTRVOnmi"
      },
      "source": [
        "### The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTGiMe_Onmi"
      },
      "source": [
        "<img alt=\"Phone with Query\" width=\"400\" caption=\"A question about a product and the corresponding review (the answer span is underlined)\" src=\"images/chapter07_phone.png\" id=\"phone\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEE1ZNqVOnmi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "cb616698-3763-4463-dae0-a1c371039ae0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-52658feab95d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dataset_config_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdomains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_config_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subjqa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "domains = get_dataset_config_names(\"subjqa\")\n",
        "domains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGlqhCHVOnmj"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from datasets import load_dataset\n",
        "\n",
        "subjqa = load_dataset(\"subjqa\", name=\"electronics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlqHZVHSOnmj"
      },
      "outputs": [],
      "source": [
        "print(subjqa[\"train\"][\"answers\"][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghLHYe8eOnmj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
        "\n",
        "for split, df in dfs.items():\n",
        "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay_mSnHdOnmk"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "qa_cols = [\"title\", \"question\", \"answers.text\",\n",
        "           \"answers.answer_start\", \"context\"]\n",
        "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
        "sample_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDEG9rfsOnmk"
      },
      "outputs": [],
      "source": [
        "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
        "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
        "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8ji1hUbOnml"
      },
      "outputs": [],
      "source": [
        "counts = {}\n",
        "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
        "\n",
        "for q in question_types:\n",
        "    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
        "\n",
        "pd.Series(counts).sort_values().plot.barh()\n",
        "plt.title(\"Frequency of Question Types\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX1tBuL_Onml"
      },
      "outputs": [],
      "source": [
        "for question_type in [\"How\", \"What\", \"Is\"]:\n",
        "    for question in (\n",
        "        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n",
        "        .sample(n=3, random_state=42)['question']):\n",
        "        print(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AInIpxcOOnml"
      },
      "source": [
        "### Sidebar: The Stanford Question Answering Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98VZ3a9kOnmm"
      },
      "source": [
        "<img alt=\"SQuAD SotA\" width=\"600\" caption=\"Progress on the SQuAD 2.0 benchmark (image from Papers with Code)\" src=\"images/chapter07_squad-sota.png\" id=\"squad-sota\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ApCF1dvOnmm"
      },
      "source": [
        "### End sidebar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WySpigWUOnmm"
      },
      "source": [
        "### Extracting Answers from Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dslS5gFVOnmm"
      },
      "source": [
        "#### Span classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYM_VxoaOnmm"
      },
      "source": [
        "<img alt=\"QA Head\" caption=\"The span classification head for QA tasks\" src=\"images/chapter07_qa-head.png\" id=\"qa-head\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZYHITW8Onmm"
      },
      "source": [
        "<img alt=\"SQuAD models\" width=\"600\" caption=\"A selection of extractive QA models on the Hugging Face Hub\" src=\"images/chapter07_squad-models.png\" id=\"squad-models\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkDi3eKTOnmm"
      },
      "source": [
        "#### Tokenizing text for QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG5_IVrBOnmn"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXfBBfF7Onmn"
      },
      "outputs": [],
      "source": [
        "question = \"How much music can this hold?\"\n",
        "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\n",
        "file size.\"\"\"\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcC_JxUZOnmn"
      },
      "outputs": [],
      "source": [
        "#hide_input\n",
        "input_df = pd.DataFrame.from_dict(tokenizer(question, context), orient=\"index\")\n",
        "input_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrje7OVhOnmn"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiKFUGltOnmn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL97dqsPOnmn"
      },
      "outputs": [],
      "source": [
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jFsE1jsOnmo"
      },
      "outputs": [],
      "source": [
        "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
        "print(f\"Start logits shape: {start_logits.size()}\")\n",
        "print(f\"End logits shape: {end_logits.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6b4GVhNOnmo"
      },
      "outputs": [],
      "source": [
        "#hide_input\n",
        "#id qa-scores\n",
        "#caption Predicted logits for the start and end tokens; the token with the highest score is colored in orange\n",
        "\n",
        "# The idea for this visualisation comes from https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s_scores = start_logits.detach().numpy().flatten()\n",
        "e_scores = end_logits.detach().numpy().flatten()\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
        "colors = [\"C0\" if s != np.max(s_scores) else \"C1\" for s in s_scores]\n",
        "ax1.bar(x=tokens, height=s_scores, color=colors)\n",
        "ax1.set_ylabel(\"Start Scores\")\n",
        "colors = [\"C0\" if s != np.max(e_scores) else \"C1\" for s in e_scores]\n",
        "ax2.bar(x=tokens, height=e_scores, color=colors)\n",
        "ax2.set_ylabel(\"End Scores\")\n",
        "plt.xticks(rotation=\"vertical\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKarPaCeOnmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "start_idx = torch.argmax(start_logits)\n",
        "end_idx = torch.argmax(end_logits) + 1\n",
        "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
        "answer = tokenizer.decode(answer_span)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AtNSTslOnmo"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "pipe(question=question, context=context, topk=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSSSTHLlOnmo"
      },
      "outputs": [],
      "source": [
        "pipe(question=\"Why is there no data?\", context=context,\n",
        "     handle_impossible_answer=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUy9rTvBOnmp"
      },
      "source": [
        "#### Dealing with long passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r6WFmoeOnmp"
      },
      "outputs": [],
      "source": [
        "#hide_input\n",
        "#id subjqa-dist\n",
        "#caption Distribution of tokens for each question-context pair in the SubjQA training set\n",
        "def compute_input_length(row):\n",
        "    inputs = tokenizer(row[\"question\"], row[\"context\"])\n",
        "    return len(inputs[\"input_ids\"])\n",
        "\n",
        "dfs[\"train\"][\"n_tokens\"] = dfs[\"train\"].apply(compute_input_length, axis=1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "dfs[\"train\"][\"n_tokens\"].hist(bins=100, grid=False, ec=\"C0\", ax=ax)\n",
        "plt.xlabel(\"Number of tokens in question-context pair\")\n",
        "ax.axvline(x=512, ymin=0, ymax=1, linestyle=\"--\", color=\"C1\",\n",
        "           label=\"Maximum sequence length\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGt0UTZNOnmp"
      },
      "source": [
        "<img alt=\"Sliding window\" caption=\"How the sliding window creates multiple question-context pairs for long documents—the first bar corresponds to the question, while the second bar is the context captured in each window\" src=\"images/chapter07_sliding-window.png\" id=\"sliding-window\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkmWjhqfOnmt"
      },
      "outputs": [],
      "source": [
        "example = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\n",
        "tokenized_example = tokenizer(example[\"question\"], example[\"context\"],\n",
        "                              return_overflowing_tokens=True, max_length=100,\n",
        "                              stride=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_XpCeQWOnmt"
      },
      "outputs": [],
      "source": [
        "for idx, window in enumerate(tokenized_example[\"input_ids\"]):\n",
        "    print(f\"Window #{idx} has {len(window)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCroCCb4Onmu"
      },
      "outputs": [],
      "source": [
        "for window in tokenized_example[\"input_ids\"]:\n",
        "    print(f\"{tokenizer.decode(window)} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFyZhkShOnmu"
      },
      "source": [
        "### Using Haystack to Build a QA Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaKlWifXOnmu"
      },
      "source": [
        "<img alt=\"QA Architecture\" caption=\"The retriever-reader architecture for modern QA systems\" src=\"images/chapter07_retriever-reader.png\" id=\"retriever-reader\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A26ljUubOnmu"
      },
      "source": [
        "#### Initializing a document store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sWit_gFOnmu"
      },
      "outputs": [],
      "source": [
        "url = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\n",
        "elasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n",
        "!wget -nc -q {url}\n",
        "!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl63eR2VOnmu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# Run Elasticsearch as a background process\n",
        "!chown -R daemon:daemon elasticsearch-7.9.2\n",
        "es_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "                  stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "# Wait until Elasticsearch has started\n",
        "!sleep 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUsqaU07Onmu"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "# Alternative if Docker is installed\n",
        "from haystack.utils import launch_es\n",
        "\n",
        "launch_es()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTNggtlkOnmv"
      },
      "outputs": [],
      "source": [
        "!curl -X GET \"localhost:9200/?pretty\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyZ1Vy8NOnmv"
      },
      "outputs": [],
      "source": [
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "\n",
        "# Return the document embedding for later use with dense retriever\n",
        "document_store = ElasticsearchDocumentStore(return_embedding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR7xLda5Onmv"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "# It's a good idea to flush Elasticsearch with each notebook restart\n",
        "if len(document_store.get_all_documents()) or len(document_store.get_all_labels()) > 0:\n",
        "    document_store.delete_documents(\"document\")\n",
        "    document_store.delete_documents(\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVivudIdOnmv"
      },
      "outputs": [],
      "source": [
        "for split, df in dfs.items():\n",
        "    # Exclude duplicate reviews\n",
        "    docs = [{\"text\": row[\"context\"],\n",
        "             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\n",
        "                     \"split\": split}}\n",
        "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
        "    document_store.write_documents(docs, index=\"document\")\n",
        "\n",
        "print(f\"Loaded {document_store.get_document_count()} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebfX0H0EOnmv"
      },
      "source": [
        "#### Initializing a retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfinkh1zOnmv"
      },
      "outputs": [],
      "source": [
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "\n",
        "es_retriever = ElasticsearchRetriever(document_store=document_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvttoaQ7Onmw"
      },
      "outputs": [],
      "source": [
        "item_id = \"B0074BW614\"\n",
        "query = \"Is it good for reading?\"\n",
        "retrieved_docs = es_retriever.retrieve(\n",
        "    query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVRf2CsJOnmw"
      },
      "outputs": [],
      "source": [
        "print(retrieved_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2GIZoqKOnmw"
      },
      "source": [
        "#### Initializing a reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkPFOxc0Onmw"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from haystack.reader.farm import FARMReader\n",
        "\n",
        "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
        "max_seq_length, doc_stride = 384, 128\n",
        "reader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,\n",
        "                    max_seq_len=max_seq_length, doc_stride=doc_stride,\n",
        "                    return_no_answer=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7X70h1kOnmw"
      },
      "outputs": [],
      "source": [
        "print(reader.predict_on_texts(question=question, texts=[context], top_k=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM0Op7HCOnmx"
      },
      "source": [
        "#### Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg0cCm8FOnmx"
      },
      "outputs": [],
      "source": [
        "from haystack.pipeline import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader, es_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDDsxUkBOnmx"
      },
      "outputs": [],
      "source": [
        "n_answers = 3\n",
        "preds = pipe.run(query=query, top_k_retriever=3, top_k_reader=n_answers,\n",
        "                 filters={\"item_id\": [item_id], \"split\":[\"train\"]})\n",
        "\n",
        "print(f\"Question: {preds['query']} \\n\")\n",
        "for idx in range(n_answers):\n",
        "    print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")\n",
        "    print(f\"Review snippet: ...{preds['answers'][idx]['context']}...\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp5engNtOnmx"
      },
      "source": [
        "## Improving Our QA Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KoDcRAxOnmx"
      },
      "source": [
        "### Evaluating the Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dFybnvzDOnmx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "1a3b7087-f877-4a50-acae-1478c13e3896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: haystack in /usr/local/lib/python3.10/dist-packages (0.42)\n",
            "Requirement already satisfied: construct<2.8 in /usr/local/lib/python3.10/dist-packages (from haystack) (2.5.3)\n",
            "Requirement already satisfied: pefile in /usr/local/lib/python3.10/dist-packages (from haystack) (2023.2.7)\n",
            "Requirement already satisfied: python-ptrace>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from haystack) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from construct<2.8->haystack) (1.16.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'haystack.pipeline'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ce04db787a16>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install haystack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvalDocuments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'haystack.pipeline'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install haystack\n",
        "\n",
        "from haystack.pipeline import Pipeline\n",
        "from haystack.eval import EvalDocuments\n",
        "\n",
        "class EvalRetrieverPipeline:\n",
        "    def __init__(self, retriever):\n",
        "        self.retriever = retriever\n",
        "        self.eval_retriever = EvalDocuments()\n",
        "        pipe = Pipeline()\n",
        "        pipe.add_node(component=self.retriever, name=\"ESRetriever\",\n",
        "                      inputs=[\"Query\"])\n",
        "        pipe.add_node(component=self.eval_retriever, name=\"EvalRetriever\",\n",
        "                      inputs=[\"ESRetriever\"])\n",
        "        self.pipeline = pipe\n",
        "\n",
        "\n",
        "pipe = EvalRetrieverPipeline(es_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm6sx1cqOnmx"
      },
      "outputs": [],
      "source": [
        "from haystack import Label\n",
        "\n",
        "labels = []\n",
        "for i, row in dfs[\"test\"].iterrows():\n",
        "    # Metadata used for filtering in the Retriever\n",
        "    meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n",
        "    # Populate labels for questions with answers\n",
        "    if len(row[\"answers.text\"]):\n",
        "        for answer in row[\"answers.text\"]:\n",
        "            label = Label(\n",
        "                question=row[\"question\"], answer=answer, id=i, origin=row[\"id\"],\n",
        "                meta=meta, is_correct_answer=True, is_correct_document=True,\n",
        "                no_answer=False)\n",
        "            labels.append(label)\n",
        "    # Populate labels for questions without answers\n",
        "    else:\n",
        "        label = Label(\n",
        "            question=row[\"question\"], answer=\"\", id=i, origin=row[\"id\"],\n",
        "            meta=meta, is_correct_answer=True, is_correct_document=True,\n",
        "            no_answer=True)\n",
        "        labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EXHWuR2Onmy"
      },
      "outputs": [],
      "source": [
        "print(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0oxlvGCOnmy"
      },
      "outputs": [],
      "source": [
        "document_store.write_labels(labels, index=\"label\")\n",
        "print(f\"\"\"Loaded {document_store.get_label_count(index=\"label\")} \\\n",
        "question-answer pairs\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M5Unu0aOnmy"
      },
      "outputs": [],
      "source": [
        "labels_agg = document_store.get_all_labels_aggregated(\n",
        "    index=\"label\",\n",
        "    open_domain=True,\n",
        "    aggregate_by_meta=[\"item_id\"]\n",
        ")\n",
        "print(len(labels_agg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DztgQ1KOnmy"
      },
      "outputs": [],
      "source": [
        "print(labels_agg[109])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IylNy1T7Onmy"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(pipeline, top_k_retriever=10, top_k_reader=4):\n",
        "    for l in labels_agg:\n",
        "        _ = pipeline.pipeline.run(\n",
        "            query=l.question,\n",
        "            top_k_retriever=top_k_retriever,\n",
        "            top_k_reader=top_k_reader,\n",
        "            top_k_eval_documents=top_k_retriever,\n",
        "            labels=l,\n",
        "            filters={\"item_id\": [l.meta[\"item_id\"]], \"split\": [\"test\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap0qn3CHOnmz"
      },
      "outputs": [],
      "source": [
        "run_pipeline(pipe, top_k_retriever=3)\n",
        "print(f\"Recall@3: {pipe.eval_retriever.recall:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AL8on83Onmz"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "def evaluate_retriever(retriever, topk_values = [1,3,5,10,20]):\n",
        "    topk_results = {}\n",
        "\n",
        "    for topk in topk_values:\n",
        "        # Create Pipeline\n",
        "        p = EvalRetrieverPipeline(retriever)\n",
        "        # Loop over each question-answers pair in test set\n",
        "        run_pipeline(p, top_k_retriever=topk)\n",
        "        # Get metrics\n",
        "        topk_results[topk] = {\"recall\": p.eval_retriever.recall}\n",
        "\n",
        "    return pd.DataFrame.from_dict(topk_results, orient=\"index\")\n",
        "\n",
        "\n",
        "es_topk_df = evaluate_retriever(es_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYXChlyNOnmz"
      },
      "outputs": [],
      "source": [
        "def plot_retriever_eval(dfs, retriever_names):\n",
        "    fig, ax = plt.subplots()\n",
        "    for df, retriever_name in zip(dfs, retriever_names):\n",
        "        df.plot(y=\"recall\", ax=ax, label=retriever_name)\n",
        "    plt.xticks(df.index)\n",
        "    plt.ylabel(\"Top-k Recall\")\n",
        "    plt.xlabel(\"k\")\n",
        "    plt.show()\n",
        "\n",
        "plot_retriever_eval([es_topk_df], [\"BM25\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-QaYH9_Onmz"
      },
      "source": [
        "#### Dense Passage Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMBivMYAOnmz"
      },
      "source": [
        "<img alt=\"DPR Architecture\" caption=\"DPR's bi-encoder architecture for computing the relevance of a document and query\" src=\"images/chapter07_dpr.png\" id=\"dpr\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL7HnMRMOnmz"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "\n",
        "dpr_retriever = DensePassageRetriever(document_store=document_store,\n",
        "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
        "    embed_title=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82nZEu2yOnmz"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "document_store.update_embeddings(retriever=dpr_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAhBO_OeOnm0"
      },
      "outputs": [],
      "source": [
        "dpr_topk_df = evaluate_retriever(dpr_retriever)\n",
        "plot_retriever_eval([es_topk_df, dpr_topk_df], [\"BM25\", \"DPR\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGJ9aR5ROnm0"
      },
      "source": [
        "### Evaluating the Reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71C-j4spOnm0"
      },
      "outputs": [],
      "source": [
        "from farm.evaluation.squad_evaluation import compute_f1, compute_exact\n",
        "\n",
        "pred = \"about 6000 hours\"\n",
        "label = \"6000 hours\"\n",
        "print(f\"EM: {compute_exact(label, pred)}\")\n",
        "print(f\"F1: {compute_f1(label, pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsMAlcNDOnm0"
      },
      "outputs": [],
      "source": [
        "pred = \"about 6000 dollars\"\n",
        "print(f\"EM: {compute_exact(label, pred)}\")\n",
        "print(f\"F1: {compute_f1(label, pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXjtpnLdOnm0"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from haystack.eval import EvalAnswers\n",
        "\n",
        "def evaluate_reader(reader):\n",
        "    score_keys = ['top_1_em', 'top_1_f1']\n",
        "    eval_reader = EvalAnswers(skip_incorrect_retrieval=False)\n",
        "    pipe = Pipeline()\n",
        "    pipe.add_node(component=reader, name=\"QAReader\", inputs=[\"Query\"])\n",
        "    pipe.add_node(component=eval_reader, name=\"EvalReader\", inputs=[\"QAReader\"])\n",
        "\n",
        "    for l in labels_agg:\n",
        "        doc = document_store.query(l.question,\n",
        "                                   filters={\"question_id\":[l.origin]})\n",
        "        _ = pipe.run(query=l.question, documents=doc, labels=l)\n",
        "\n",
        "    return {k:v for k,v in eval_reader.__dict__.items() if k in score_keys}\n",
        "\n",
        "reader_eval = {}\n",
        "reader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWCcY39hOnm0"
      },
      "outputs": [],
      "source": [
        "def plot_reader_eval(reader_eval):\n",
        "    fig, ax = plt.subplots()\n",
        "    df = pd.DataFrame.from_dict(reader_eval)\n",
        "    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n",
        "    ax.set_xticklabels([\"EM\", \"F1\"])\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_reader_eval(reader_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFYW6z2LOnm1"
      },
      "source": [
        "### Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RYUe47Onm1"
      },
      "source": [
        "<img alt=\"SQuAD Schema\" caption=\"Visualization of the SQuAD JSON format\" src=\"images/chapter07_squad-schema.png\" id=\"squad-schema\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnX_fbo2Onm1"
      },
      "outputs": [],
      "source": [
        "def create_paragraphs(df):\n",
        "    paragraphs = []\n",
        "    id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\n",
        "    for review_id, review in id2context.items():\n",
        "        qas = []\n",
        "        # Filter for all question-answer pairs about a specific context\n",
        "        review_df = df.query(f\"review_id == '{review_id}'\")\n",
        "        id2question = dict(zip(review_df[\"id\"], review_df[\"question\"]))\n",
        "        # Build up the qas array\n",
        "        for qid, question in id2question.items():\n",
        "            # Filter for a single question ID\n",
        "            question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n",
        "            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n",
        "            ans_text = question_df[\"answers.text\"][0].tolist()\n",
        "            # Fill answerable questions\n",
        "            if len(ans_start_idxs):\n",
        "                answers = [\n",
        "                    {\"text\": text, \"answer_start\": answer_start}\n",
        "                    for text, answer_start in zip(ans_text, ans_start_idxs)]\n",
        "                is_impossible = False\n",
        "            else:\n",
        "                answers = []\n",
        "                is_impossible = True\n",
        "            # Add question-answer pairs to qas\n",
        "            qas.append({\"question\": question, \"id\": qid,\n",
        "                        \"is_impossible\": is_impossible, \"answers\": answers})\n",
        "        # Add context and question-answer pairs to paragraphs\n",
        "        paragraphs.append({\"qas\": qas, \"context\": review})\n",
        "    return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDqRS-caOnm1"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "product = dfs[\"train\"].query(\"title == 'B00001P4ZH'\")\n",
        "create_paragraphs(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uByJWDXhOnm1"
      },
      "source": [
        "```python\n",
        "[{'qas': [{'question': 'How is the bass?',\n",
        "    'id': '2543d296da9766d8d17d040ecc781699',\n",
        "    'is_impossible': True,\n",
        "    'answers': []}],\n",
        "  'context': 'I have had Koss headphones ...',\n",
        "    'id': 'd476830bf9282e2b9033e2bb44bbb995',\n",
        "    'is_impossible': False,\n",
        "    'answers': [{'text': 'Bass is weak as expected', 'answer_start': 1302},\n",
        "     {'text': 'Bass is weak as expected, even with EQ adjusted up',\n",
        "      'answer_start': 1302}]}],\n",
        "  'context': 'To anyone who hasn\\'t tried all ...'},\n",
        " {'qas': [{'question': 'How is the bass?',\n",
        "    'id': '455575557886d6dfeea5aa19577e5de4',\n",
        "    'is_impossible': False,\n",
        "    'answers': [{'text': 'The only fault in the sound is the bass',\n",
        "      'answer_start': 650}]}],\n",
        "  'context': \"I have had many sub-$100 headphones ...\"}]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi_MYXlrOnm1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def convert_to_squad(dfs):\n",
        "    for split, df in dfs.items():\n",
        "        subjqa_data = {}\n",
        "        # Create `paragraphs` for each product ID\n",
        "        groups = (df.groupby(\"title\").apply(create_paragraphs)\n",
        "            .to_frame(name=\"paragraphs\").reset_index())\n",
        "        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n",
        "        # Save the result to disk\n",
        "        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\n",
        "            json.dump(subjqa_data, f)\n",
        "\n",
        "convert_to_squad(dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFToZrsBOnm2"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "train_filename = \"electronics-train.json\"\n",
        "dev_filename = \"electronics-validation.json\"\n",
        "\n",
        "reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n",
        "             train_filename=train_filename, dev_filename=dev_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnVpWxW8Onm2"
      },
      "outputs": [],
      "source": [
        "reader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader)\n",
        "plot_reader_eval(reader_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie1FZjCWOnm2"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "minilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "minilm_reader = FARMReader(model_name_or_path=minilm_ckpt, progress_bar=False,\n",
        "                           max_seq_len=max_seq_length, doc_stride=doc_stride,\n",
        "                           return_no_answer=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5paMJRzGOnm2"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "minilm_reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n",
        "             train_filename=train_filename, dev_filename=dev_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9SQ3qqdOnm2"
      },
      "outputs": [],
      "source": [
        "reader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader)\n",
        "plot_reader_eval(reader_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HySrZF0cOnm2"
      },
      "source": [
        "### Evaluating the Whole QA Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8uK2UoBOnm3"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "# Initialize retriever pipeline\n",
        "pipe = EvalRetrieverPipeline(es_retriever)\n",
        "# Add nodes for reader\n",
        "eval_reader = EvalAnswers()\n",
        "pipe.pipeline.add_node(component=reader, name=\"QAReader\",\n",
        "              inputs=[\"EvalRetriever\"])\n",
        "pipe.pipeline.add_node(component=eval_reader, name=\"EvalReader\",\n",
        "              inputs=[\"QAReader\"])\n",
        "# Evaluate!\n",
        "run_pipeline(pipe)\n",
        "# Extract metrics from reader\n",
        "reader_eval[\"QA Pipeline (top-1)\"] = {\n",
        "    k:v for k,v in eval_reader.__dict__.items()\n",
        "    if k in [\"top_1_em\", \"top_1_f1\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0X__g3gOnm3"
      },
      "outputs": [],
      "source": [
        "#hide_input\n",
        "#id reader-vs-pipeline\n",
        "#caption Comparison of EM and _F_~1~ scores for the reader against the whole QA pipeline\n",
        "plot_reader_eval({\"Reader\": reader_eval[\"Fine-tune on SQuAD + SubjQA\"],\n",
        "                  \"QA pipeline (top-1)\": reader_eval[\"QA Pipeline (top-1)\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXUeS4ZvOnm3"
      },
      "source": [
        "## Going Beyond Extractive QA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDzdkYB6Onm3"
      },
      "source": [
        "<img alt=\"RAG Architecture\" width=\"600\" caption=\"The RAG architecture for fine-tuning a retriever and generator end-to-end (courtesy of Ethan Perez)\" src=\"images/chapter07_rag-architecture.png\" id=\"rag\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5Y3NtTuOnm3"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from haystack.generator.transformers import RAGenerator\n",
        "\n",
        "generator = RAGenerator(model_name_or_path=\"facebook/rag-token-nq\",\n",
        "                        embed_title=False, num_beams=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knARy9yJOnm3"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from haystack.pipeline import GenerativeQAPipeline\n",
        "\n",
        "pipe = GenerativeQAPipeline(generator=generator, retriever=dpr_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSY2fr0hOnm3"
      },
      "outputs": [],
      "source": [
        "def generate_answers(query, top_k_generator=3):\n",
        "    preds = pipe.run(query=query, top_k_generator=top_k_generator,\n",
        "                     top_k_retriever=5, filters={\"item_id\":[\"B0074BW614\"]})\n",
        "    print(f\"Question: {preds['query']} \\n\")\n",
        "    for idx in range(top_k_generator):\n",
        "        print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyu7k4PpOnm4"
      },
      "outputs": [],
      "source": [
        "generate_answers(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzy2oEr9Onm4"
      },
      "outputs": [],
      "source": [
        "generate_answers(\"What is the main drawback?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPbYhMwnOnm4"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4ik24HIOnm4"
      },
      "source": [
        "<img alt=\"QA Pyramid\" caption=\"The QA hierarchy of needs\" src=\"images/chapter07_qa-pyramid.png\" id=\"qa-pyramid\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0ceDx-WOnm4"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}